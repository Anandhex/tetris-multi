{
    "name": "root",
    "gauges": {
        "TetrisAgent.Policy.Entropy.mean": {
            "value": 3.641873836517334,
            "min": 3.6311638355255127,
            "max": 3.6888718605041504,
            "count": 50
        },
        "TetrisAgent.Policy.Entropy.sum": {
            "value": 37616.9140625,
            "min": 22166.48828125,
            "max": 50598.0234375,
            "count": 50
        },
        "TetrisAgent.action-rewarded.survival.mean": {
            "value": 0.009999999776482582,
            "min": 0.009999999776482582,
            "max": 0.009999999776482582,
            "count": 50
        },
        "TetrisAgent.action-rewarded.survival.sum": {
            "value": 103.28999769128859,
            "min": 60.82999864034355,
            "max": 138.8799968957901,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tetromino_types.mean": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tetromino_types.sum": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.board_height.mean": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.board_height.sum": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.hole_penalty_weight.mean": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.hole_penalty_weight.sum": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.board_preset.mean": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.board_preset.sum": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.clearReward.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.clearReward.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.enable_t_spins.mean": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.enable_t_spins.sum": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.perfectClearBonus.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.perfectClearBonus.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.stagnationPenaltyFactor.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.stagnationPenaltyFactor.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.comboMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.comboMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tetrisClearRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tetrisClearRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tripleLineClearRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tripleLineClearRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.doubleLineClearRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.doubleLineClearRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tSpinReward.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.tSpinReward.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.iPieceGapFillBonus.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.iPieceGapFillBonus.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.deathPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.deathPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.stackHeightPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.stackHeightPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.holeCreationPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.holeCreationPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.uselessRotationPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.uselessRotationPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.idleActionPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.idleActionPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.roughnessRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.roughnessRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.roughnessPenaltyMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.roughnessPenaltyMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.holeFillReward.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.holeFillReward.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.wellRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.wellRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.iPieceInWellBonus.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.iPieceInWellBonus.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.maxWellRewardCap.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.maxWellRewardCap.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityPenaltyMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityPenaltyMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.action-rewarded.placement-quality.mean": {
            "value": -0.4654838788893915,
            "min": -0.5598113328429326,
            "max": 0.05774194380689052,
            "count": 50
        },
        "TetrisAgent.action-rewarded.placement-quality.sum": {
            "value": -28.860000491142273,
            "min": -44.16000082343817,
            "max": 5.370000774040818,
            "count": 50
        },
        "TetrisAgent.action-rewarded.roughness-penalty.mean": {
            "value": -0.0010767195668045639,
            "min": -0.0011003039278179169,
            "max": -0.0006030534177695095,
            "count": 50
        },
        "TetrisAgent.action-rewarded.roughness-penalty.sum": {
            "value": -0.05814285660744645,
            "min": -0.09524999605491757,
            "max": -0.02385714247066062,
            "count": 50
        },
        "TetrisAgent.action-rewarded.hole-creation.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.action-rewarded.hole-creation.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 50
        },
        "TetrisAgent.action-rewarded.height-penalty.mean": {
            "value": -0.007698652593940835,
            "min": -0.009947048125526874,
            "max": -0.0071212165246358,
            "count": 50
        },
        "TetrisAgent.action-rewarded.height-penalty.sum": {
            "value": -79.51938264281489,
            "min": -129.55570861743763,
            "max": -43.31836011935957,
            "count": 50
        },
        "TetrisAgent.action-rewarded.accessibility-improvement.mean": {
            "value": 0.016403226174354073,
            "min": 0.013821902446264078,
            "max": 0.019170462762793668,
            "count": 50
        },
        "TetrisAgent.action-rewarded.accessibility-improvement.sum": {
            "value": 1.0170000228099525,
            "min": 0.5638857372105122,
            "max": 2.1344501595012844,
            "count": 50
        },
        "TetrisAgent.action-rewarded.hole-fill.mean": {
            "value": 6.798239467849194,
            "min": 4.844757227793984,
            "max": 7.792281950680201,
            "count": 50
        },
        "TetrisAgent.action-rewarded.hole-fill.sum": {
            "value": 1930.7000088691711,
            "min": 972.0000284910202,
            "max": 2513.0000277757645,
            "count": 50
        },
        "TetrisAgent.action-rewarded.accessibility-loss.mean": {
            "value": -0.0023994226598961638,
            "min": -0.0029690004241031904,
            "max": -0.0022825001546556856,
            "count": 50
        },
        "TetrisAgent.action-rewarded.accessibility-loss.sum": {
            "value": -0.2375428433297202,
            "min": -0.4925875416956842,
            "max": -0.12657142477110028,
            "count": 50
        },
        "TetrisAgent.action-rewarded.roughness-improvement.mean": {
            "value": 0.01491836794164209,
            "min": 0.01121538531465026,
            "max": 0.018000000456733438,
            "count": 50
        },
        "TetrisAgent.action-rewarded.roughness-improvement.sum": {
            "value": 1.5664286338724196,
            "min": 0.7242857506498694,
            "max": 2.906250041909516,
            "count": 50
        },
        "TetrisAgent.action-rewarded.well-formation.mean": {
            "value": 0.9364827350284808,
            "min": 0.8311889351433573,
            "max": 0.9839125770964703,
            "count": 50
        },
        "TetrisAgent.action-rewarded.well-formation.sum": {
            "value": 2359.000009536743,
            "min": 869.6000043749809,
            "max": 7411.4000135064125,
            "count": 50
        },
        "TetrisAgent.action-rewarded.stagnation-penalty.mean": {
            "value": -0.09636944831823546,
            "min": -0.09745841188001511,
            "max": -0.09212118157612091,
            "count": 50
        },
        "TetrisAgent.action-rewarded.stagnation-penalty.sum": {
            "value": -919.7500147492392,
            "min": -1237.4000199299771,
            "max": -548.500008769799,
            "count": 50
        },
        "TetrisAgent.action-rewarded.i-piece-well-setup.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 50
        },
        "TetrisAgent.action-rewarded.i-piece-well-setup.sum": {
            "value": 125.0,
            "min": 92.0,
            "max": 1984.0,
            "count": 50
        },
        "TetrisAgent.Step.mean": {
            "value": 499951.0,
            "min": 9596.0,
            "max": 499951.0,
            "count": 50
        },
        "TetrisAgent.Step.sum": {
            "value": 499951.0,
            "min": 9596.0,
            "max": 499951.0,
            "count": 50
        },
        "TetrisAgent.Policy.ExtrinsicValueEstimate.mean": {
            "value": 134.50332641601562,
            "min": -0.00811776053160429,
            "max": 140.44728088378906,
            "count": 50
        },
        "TetrisAgent.Policy.ExtrinsicValueEstimate.sum": {
            "value": 2959.0732421875,
            "min": -0.19482626020908356,
            "max": 3311.873046875,
            "count": 50
        },
        "TetrisAgent.Policy.CuriosityValueEstimate.mean": {
            "value": 0.563896894454956,
            "min": 0.005083067808300257,
            "max": 1.0748108625411987,
            "count": 50
        },
        "TetrisAgent.Policy.CuriosityValueEstimate.sum": {
            "value": 12.405731201171875,
            "min": 0.12199363112449646,
            "max": 24.72064971923828,
            "count": 50
        },
        "TetrisAgent.action-rewarded.strategic-placement.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 30
        },
        "TetrisAgent.action-rewarded.strategic-placement.sum": {
            "value": 0.5,
            "min": 0.5,
            "max": 4.0,
            "count": 30
        },
        "TetrisAgent.action-rewarded.partial-row-fill.mean": {
            "value": 0.00812828753809737,
            "min": 0.007201845321542183,
            "max": 0.008250922349103995,
            "count": 50
        },
        "TetrisAgent.action-rewarded.partial-row-fill.sum": {
            "value": 179.562000004109,
            "min": 10.307000225875527,
            "max": 254.69499373389408,
            "count": 50
        },
        "TetrisAgent.action-rewarded.horizontal-stack.mean": {
            "value": 1.0693677993997484,
            "min": 0.5,
            "max": 1.4172283412789741,
            "count": 50
        },
        "TetrisAgent.action-rewarded.horizontal-stack.sum": {
            "value": 11045.5,
            "min": 439.5,
            "max": 15707.0,
            "count": 50
        },
        "TetrisAgent.reward.cumulative.mean": {
            "value": 1646.0528215680804,
            "min": 262.2979685465495,
            "max": 2888.74551827567,
            "count": 50
        },
        "TetrisAgent.reward.cumulative.sum": {
            "value": 11522.369750976562,
            "min": 786.8939056396484,
            "max": 27167.97265625,
            "count": 50
        },
        "TetrisAgent.curriculum.board-height.mean": {
            "value": 14.0,
            "min": 8.0,
            "max": 14.0,
            "count": 50
        },
        "TetrisAgent.curriculum.board-height.sum": {
            "value": 98.0,
            "min": 20.0,
            "max": 154.0,
            "count": 50
        },
        "TetrisAgent.curriculum.hole-penalty-weight.mean": {
            "value": 0.10000000149011612,
            "min": 0.019999999552965164,
            "max": 0.10000000149011612,
            "count": 50
        },
        "TetrisAgent.curriculum.hole-penalty-weight.sum": {
            "value": 0.7000000104308128,
            "min": 0.05999999865889549,
            "max": 1.1000000163912773,
            "count": 50
        },
        "TetrisAgent.curriculum.tetromino-types.mean": {
            "value": 3.0,
            "min": 1.0,
            "max": 3.0,
            "count": 50
        },
        "TetrisAgent.curriculum.tetromino-types.sum": {
            "value": 21.0,
            "min": 3.0,
            "max": 33.0,
            "count": 50
        },
        "TetrisAgent.curriculum.board-preset.mean": {
            "value": 3.0,
            "min": 1.0,
            "max": 3.0,
            "count": 50
        },
        "TetrisAgent.curriculum.board-preset.sum": {
            "value": 21.0,
            "min": 3.0,
            "max": 33.0,
            "count": 50
        },
        "TetrisAgent.metrics.current-holes.mean": {
            "value": 16.571428571428573,
            "min": 3.3333333333333335,
            "max": 32.666666666666664,
            "count": 50
        },
        "TetrisAgent.metrics.current-holes.sum": {
            "value": 116.0,
            "min": 10.0,
            "max": 285.0,
            "count": 50
        },
        "TetrisAgent.metrics.stack-height.mean": {
            "value": 13.0,
            "min": 6.0,
            "max": 14.0,
            "count": 50
        },
        "TetrisAgent.metrics.stack-height.sum": {
            "value": 91.0,
            "min": 19.0,
            "max": 144.0,
            "count": 50
        },
        "TetrisAgent.metrics.surface-roughness.mean": {
            "value": 0.11530612329287189,
            "min": 0.08500000089406967,
            "max": 0.2007936533126566,
            "count": 50
        },
        "TetrisAgent.metrics.surface-roughness.sum": {
            "value": 0.8071428630501032,
            "min": 0.17000000178813934,
            "max": 2.1928571946918964,
            "count": 50
        },
        "TetrisAgent.Environment.EpisodeLength.mean": {
            "value": 1635.1666666666667,
            "min": 762.9166666666666,
            "max": 2149.0,
            "count": 50
        },
        "TetrisAgent.Environment.EpisodeLength.sum": {
            "value": 9811.0,
            "min": 5513.0,
            "max": 14956.0,
            "count": 50
        },
        "TetrisAgent.Environment.CumulativeReward.mean": {
            "value": 2311.7563705444336,
            "min": 393.8675473034382,
            "max": 4102.704669189453,
            "count": 50
        },
        "TetrisAgent.Environment.CumulativeReward.sum": {
            "value": 16182.294593811035,
            "min": 4726.410567641258,
            "max": 21897.428146362305,
            "count": 50
        },
        "TetrisAgent.Policy.ExtrinsicReward.mean": {
            "value": 2311.7563705444336,
            "min": 393.8675473034382,
            "max": 4102.704669189453,
            "count": 50
        },
        "TetrisAgent.Policy.ExtrinsicReward.sum": {
            "value": 16182.294593811035,
            "min": 4726.410567641258,
            "max": 21897.428146362305,
            "count": 50
        },
        "TetrisAgent.Policy.CuriosityReward.mean": {
            "value": 8.864468757595334,
            "min": 0.0,
            "max": 50.9779227077961,
            "count": 50
        },
        "TetrisAgent.Policy.CuriosityReward.sum": {
            "value": 62.05128130316734,
            "min": 0.0,
            "max": 611.7350724935532,
            "count": 50
        },
        "TetrisAgent.action-rewarded.clear-reward.mean": {
            "value": 5.0,
            "min": 5.0,
            "max": 15.0,
            "count": 28
        },
        "TetrisAgent.action-rewarded.clear-reward.sum": {
            "value": 10.0,
            "min": 5.0,
            "max": 30.0,
            "count": 28
        },
        "TetrisAgent.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        },
        "TetrisAgent.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 50
        },
        "TetrisAgent.Losses.PolicyLoss.mean": {
            "value": 0.04551418526189081,
            "min": 0.04011695783491973,
            "max": 0.04551418526189081,
            "count": 17
        },
        "TetrisAgent.Losses.PolicyLoss.sum": {
            "value": 0.04551418526189081,
            "min": 0.04011695783491973,
            "max": 0.04551418526189081,
            "count": 17
        },
        "TetrisAgent.Losses.ValueLoss.mean": {
            "value": 95.86248492002487,
            "min": 34.76706650853157,
            "max": 126.37323290109634,
            "count": 17
        },
        "TetrisAgent.Losses.ValueLoss.sum": {
            "value": 95.86248492002487,
            "min": 34.76706650853157,
            "max": 126.37323290109634,
            "count": 17
        },
        "TetrisAgent.Policy.LearningRate.mean": {
            "value": 1.4853396151315508e-06,
            "min": 1.4853396151315508e-06,
            "max": 2.9802245734577127e-05,
            "count": 17
        },
        "TetrisAgent.Policy.LearningRate.sum": {
            "value": 1.4853396151315508e-06,
            "min": 1.4853396151315508e-06,
            "max": 2.9802245734577127e-05,
            "count": 17
        },
        "TetrisAgent.Policy.Epsilon.mean": {
            "value": 0.1026706729869917,
            "min": 0.1026706729869917,
            "max": 0.15358851511691982,
            "count": 17
        },
        "TetrisAgent.Policy.Epsilon.sum": {
            "value": 0.1026706729869917,
            "min": 0.1026706729869917,
            "max": 0.15358851511691982,
            "count": 17
        },
        "TetrisAgent.Policy.Beta.mean": {
            "value": 0.00036542671518769836,
            "min": 0.00036542671518769836,
            "max": 0.007141831561769665,
            "count": 17
        },
        "TetrisAgent.Policy.Beta.sum": {
            "value": 0.00036542671518769836,
            "min": 0.00036542671518769836,
            "max": 0.007141831561769665,
            "count": 17
        },
        "TetrisAgent.Losses.CuriosityForwardLoss.mean": {
            "value": 0.004905784330003371,
            "min": 0.0015888029249007008,
            "max": 0.012056081450282363,
            "count": 17
        },
        "TetrisAgent.Losses.CuriosityForwardLoss.sum": {
            "value": 0.004905784330003371,
            "min": 0.0015888029249007008,
            "max": 0.012056081450282363,
            "count": 17
        },
        "TetrisAgent.Losses.CuriosityInverseLoss.mean": {
            "value": 3.6431442085653543,
            "min": 3.6327762522815186,
            "max": 3.688639719411731,
            "count": 17
        },
        "TetrisAgent.Losses.CuriosityInverseLoss.sum": {
            "value": 3.6431442085653543,
            "min": 3.6327762522815186,
            "max": 3.688639719411731,
            "count": 17
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1748712387",
        "python_version": "3.9.22 | packaged by conda-forge | (main, Apr 14 2025, 23:36:04) \n[Clang 18.1.8 ]",
        "command_line_arguments": "/opt/anaconda3/envs/ml-unity/bin/mlagents-learn ./Assets/ML-Agents/Config/tetris_ml_config.yaml --run-id=tetris_curriculum_v1 --force",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.11.0",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1748714256"
    },
    "total": 1868.961814625,
    "count": 1,
    "self": 0.031950332999940656,
    "children": {
        "run_training.setup": {
            "total": 0.014321417000000003,
            "count": 1,
            "self": 0.014321417000000003
        },
        "TrainerController.start_learning": {
            "total": 1868.915542875,
            "count": 1,
            "self": 12.574477589993649,
            "children": {
                "TrainerController._reset_env": {
                    "total": 20.510806583,
                    "count": 1,
                    "self": 20.510806583
                },
                "TrainerController.advance": {
                    "total": 1835.7263292020064,
                    "count": 500485,
                    "self": 24.672054562052836,
                    "children": {
                        "env_step": {
                            "total": 1811.0542746399535,
                            "count": 500485,
                            "self": 1500.2568420408738,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 308.3560862780038,
                                    "count": 500485,
                                    "self": 7.152167267967229,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 301.20391901003654,
                                            "count": 500485,
                                            "self": 301.20391901003654
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 2.4413463210759936,
                                    "count": 500485,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 1842.875751117067,
                                            "count": 500485,
                                            "is_parallel": true,
                                            "self": 510.16335875312825,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0013023340000017924,
                                                    "count": 1,
                                                    "is_parallel": true,
                                                    "self": 0.0006143760000014709,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0006879580000003216,
                                                            "count": 2,
                                                            "is_parallel": true,
                                                            "self": 0.0006879580000003216
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 1332.7110900299388,
                                                    "count": 500485,
                                                    "is_parallel": true,
                                                    "self": 30.004719257942043,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 26.416947647933654,
                                                            "count": 500485,
                                                            "is_parallel": true,
                                                            "self": 26.416947647933654
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 1223.3448571560243,
                                                            "count": 500485,
                                                            "is_parallel": true,
                                                            "self": 1223.3448571560243
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 52.944565968038546,
                                                            "count": 500485,
                                                            "is_parallel": true,
                                                            "self": 27.586020548913154,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 25.35854541912539,
                                                                    "count": 1000970,
                                                                    "is_parallel": true,
                                                                    "self": 25.35854541912539
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 2.420900000288384e-05,
                    "count": 1,
                    "self": 2.420900000288384e-05,
                    "children": {
                        "thread_root": {
                            "total": 0.0,
                            "count": 0,
                            "is_parallel": true,
                            "self": 0.0,
                            "children": {
                                "trainer_advance": {
                                    "total": 1824.838738923189,
                                    "count": 9108324,
                                    "is_parallel": true,
                                    "self": 58.814936704208094,
                                    "children": {
                                        "process_trajectory": {
                                            "total": 1334.696498095981,
                                            "count": 9108324,
                                            "is_parallel": true,
                                            "self": 1334.486886886981,
                                            "children": {
                                                "RLTrainer._checkpoint": {
                                                    "total": 0.2096112090000588,
                                                    "count": 1,
                                                    "is_parallel": true,
                                                    "self": 0.2096112090000588
                                                }
                                            }
                                        },
                                        "_update_policy": {
                                            "total": 431.3273041229999,
                                            "count": 17,
                                            "is_parallel": true,
                                            "self": 217.4352300589849,
                                            "children": {
                                                "TorchPPOOptimizer.update": {
                                                    "total": 213.89207406401502,
                                                    "count": 10928,
                                                    "is_parallel": true,
                                                    "self": 213.89207406401502
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.10390529100004642,
                    "count": 1,
                    "self": 0.007871707000049355,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.09603358399999706,
                            "count": 1,
                            "self": 0.09603358399999706
                        }
                    }
                }
            }
        }
    }
}