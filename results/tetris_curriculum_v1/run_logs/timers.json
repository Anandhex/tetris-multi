{
    "name": "root",
    "gauges": {
        "TetrisAgent.Policy.Entropy.mean": {
            "value": 3.688871145248413,
            "min": 3.688871145248413,
            "max": 3.688871145248413,
            "count": 2
        },
        "TetrisAgent.Policy.Entropy.sum": {
            "value": 35922.2265625,
            "min": 35922.2265625,
            "max": 38224.08203125,
            "count": 2
        },
        "TetrisAgent.action-rewarded.survival.mean": {
            "value": 0.009999999776482582,
            "min": 0.009999999776482582,
            "max": 0.009999999776482582,
            "count": 2
        },
        "TetrisAgent.action-rewarded.survival.sum": {
            "value": 97.37999782338738,
            "min": 97.37999782338738,
            "max": 103.61999768391252,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tetromino_types.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tetromino_types.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.board_height.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.board_height.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.hole_penalty_weight.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.hole_penalty_weight.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.board_preset.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.board_preset.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.clearReward.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.clearReward.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.enable_t_spins.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.enable_t_spins.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.perfectClearBonus.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.perfectClearBonus.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.stagnationPenaltyFactor.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.stagnationPenaltyFactor.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.comboMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.comboMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tetrisClearRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tetrisClearRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tripleLineClearRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tripleLineClearRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.doubleLineClearRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.doubleLineClearRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tSpinReward.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.tSpinReward.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.iPieceGapFillBonus.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.iPieceGapFillBonus.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.deathPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.deathPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.stackHeightPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.stackHeightPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.holeCreationPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.holeCreationPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.uselessRotationPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.uselessRotationPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.idleActionPenalty.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.idleActionPenalty.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.roughnessRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.roughnessRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.roughnessPenaltyMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.roughnessPenaltyMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.holeFillReward.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.holeFillReward.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.wellRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.wellRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.iPieceInWellBonus.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.iPieceInWellBonus.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.maxWellRewardCap.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.maxWellRewardCap.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityRewardMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityRewardMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityPenaltyMultiplier.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Environment.LessonNumber.accessibilityPenaltyMultiplier.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.action-rewarded.placement-quality.mean": {
            "value": 0.12430146676214303,
            "min": -0.11668202129045599,
            "max": 0.12430146676214303,
            "count": 2
        },
        "TetrisAgent.action-rewarded.placement-quality.sum": {
            "value": 8.452499739825726,
            "min": -8.867833618074656,
            "max": 8.452499739825726,
            "count": 2
        },
        "TetrisAgent.action-rewarded.exploration-bonus.mean": {
            "value": 0.20000000298023224,
            "min": 0.20000000298023224,
            "max": 0.20000000298023224,
            "count": 2
        },
        "TetrisAgent.action-rewarded.exploration-bonus.sum": {
            "value": 13.600000202655792,
            "min": 13.600000202655792,
            "max": 15.20000022649765,
            "count": 2
        },
        "TetrisAgent.action-rewarded.roughness-penalty.mean": {
            "value": -0.0007407407076557937,
            "min": -0.0008416666455256443,
            "max": -0.0007407407076557937,
            "count": 2
        },
        "TetrisAgent.action-rewarded.roughness-penalty.sum": {
            "value": -0.05999999732011929,
            "min": -0.06312499841442332,
            "max": -0.05999999732011929,
            "count": 2
        },
        "TetrisAgent.action-rewarded.hole-creation.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.action-rewarded.hole-creation.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.action-rewarded.height-penalty.mean": {
            "value": -0.00990907130282351,
            "min": -0.00990907130282351,
            "max": -0.009706188644351986,
            "count": 2
        },
        "TetrisAgent.action-rewarded.height-penalty.sum": {
            "value": -81.32374818227254,
            "min": -87.31687304459047,
            "max": -81.32374818227254,
            "count": 2
        },
        "TetrisAgent.action-rewarded.accessibility-improvement.mean": {
            "value": 0.014404168524335893,
            "min": 0.014404168524335893,
            "max": 0.014551787299015337,
            "count": 2
        },
        "TetrisAgent.action-rewarded.accessibility-improvement.sum": {
            "value": 1.209950156044215,
            "min": 1.209950156044215,
            "max": 1.2223501331172884,
            "count": 2
        },
        "TetrisAgent.action-rewarded.hole-fill.mean": {
            "value": 4.736434240442838,
            "min": 4.736434240442838,
            "max": 4.871014684222747,
            "count": 2
        },
        "TetrisAgent.action-rewarded.hole-fill.sum": {
            "value": 1222.0000340342522,
            "min": 1222.0000340342522,
            "max": 1344.400052845478,
            "count": 2
        },
        "TetrisAgent.action-rewarded.well-formation.mean": {
            "value": 0.9022329569438458,
            "min": 0.8575085409163615,
            "max": 0.9022329569438458,
            "count": 2
        },
        "TetrisAgent.action-rewarded.well-formation.sum": {
            "value": 1495.0000096559525,
            "min": 1495.0000096559525,
            "max": 2010.0000199079514,
            "count": 2
        },
        "TetrisAgent.action-rewarded.i-piece-well-setup.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 2
        },
        "TetrisAgent.action-rewarded.i-piece-well-setup.sum": {
            "value": 792.5,
            "min": 792.5,
            "max": 934.0,
            "count": 2
        },
        "TetrisAgent.action-rewarded.accessibility-loss.mean": {
            "value": -0.002515972804440747,
            "min": -0.002790062024151204,
            "max": -0.002515972804440747,
            "count": 2
        },
        "TetrisAgent.action-rewarded.accessibility-loss.sum": {
            "value": -0.31701257335953414,
            "min": -0.3403875669464469,
            "max": -0.31701257335953414,
            "count": 2
        },
        "TetrisAgent.action-rewarded.roughness-improvement.mean": {
            "value": 0.016526548663747653,
            "min": 0.014431818513637127,
            "max": 0.016526548663747653,
            "count": 2
        },
        "TetrisAgent.action-rewarded.roughness-improvement.sum": {
            "value": 1.8674999990034848,
            "min": 1.8674999990034848,
            "max": 1.9050000438001007,
            "count": 2
        },
        "TetrisAgent.action-rewarded.stagnation-penalty.mean": {
            "value": -0.09495812741568262,
            "min": -0.09495812741568262,
            "max": -0.09288242610904927,
            "count": 2
        },
        "TetrisAgent.action-rewarded.stagnation-penalty.sum": {
            "value": -839.0500138449715,
            "min": -839.7500144519145,
            "max": -839.0500138449715,
            "count": 2
        },
        "TetrisAgent.action-rewarded.strategic-placement.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.5,
            "count": 2
        },
        "TetrisAgent.action-rewarded.strategic-placement.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.5,
            "count": 2
        },
        "TetrisAgent.action-rewarded.invalid-placement.mean": {
            "value": -0.10000000149011612,
            "min": -0.10000000149011612,
            "max": -0.10000000149011612,
            "count": 2
        },
        "TetrisAgent.action-rewarded.invalid-placement.sum": {
            "value": -1.0000000149011612,
            "min": -1.5000000223517418,
            "max": -1.0000000149011612,
            "count": 2
        },
        "TetrisAgent.Environment.EpisodeLength.mean": {
            "value": 1137.888888888889,
            "min": 982.3,
            "max": 1137.888888888889,
            "count": 2
        },
        "TetrisAgent.Environment.EpisodeLength.sum": {
            "value": 10241.0,
            "min": 9823.0,
            "max": 10241.0,
            "count": 2
        },
        "TetrisAgent.Step.mean": {
            "value": 19762.0,
            "min": 9833.0,
            "max": 19762.0,
            "count": 2
        },
        "TetrisAgent.Step.sum": {
            "value": 19762.0,
            "min": 9833.0,
            "max": 19762.0,
            "count": 2
        },
        "TetrisAgent.Policy.ExtrinsicValueEstimate.mean": {
            "value": 0.011061561293900013,
            "min": 0.011061561293900013,
            "max": 0.014157464727759361,
            "count": 2
        },
        "TetrisAgent.Policy.ExtrinsicValueEstimate.sum": {
            "value": 0.27653902769088745,
            "min": 0.27653902769088745,
            "max": 0.3539366126060486,
            "count": 2
        },
        "TetrisAgent.Policy.CuriosityValueEstimate.mean": {
            "value": -0.008008011616766453,
            "min": -0.008008011616766453,
            "max": -0.0005702756461687386,
            "count": 2
        },
        "TetrisAgent.Policy.CuriosityValueEstimate.sum": {
            "value": -0.20020028948783875,
            "min": -0.20020028948783875,
            "max": -0.01425689086318016,
            "count": 2
        },
        "TetrisAgent.Environment.CumulativeReward.mean": {
            "value": 562.2558510303497,
            "min": 413.73416392803193,
            "max": 562.2558510303497,
            "count": 2
        },
        "TetrisAgent.Environment.CumulativeReward.sum": {
            "value": 4498.046808242798,
            "min": 4137.341639280319,
            "max": 4498.046808242798,
            "count": 2
        },
        "TetrisAgent.Policy.ExtrinsicReward.mean": {
            "value": 562.2558510303497,
            "min": 413.73416392803193,
            "max": 562.2558510303497,
            "count": 2
        },
        "TetrisAgent.Policy.ExtrinsicReward.sum": {
            "value": 4498.046808242798,
            "min": 4137.341639280319,
            "max": 4498.046808242798,
            "count": 2
        },
        "TetrisAgent.Policy.CuriosityReward.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.Policy.CuriosityReward.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.reward.cumulative.mean": {
            "value": 455.2752914428711,
            "min": 455.2752914428711,
            "max": 557.4601867675781,
            "count": 2
        },
        "TetrisAgent.reward.cumulative.sum": {
            "value": 2731.6517486572266,
            "min": 2731.6517486572266,
            "max": 2787.3009338378906,
            "count": 2
        },
        "TetrisAgent.curriculum.board-height.mean": {
            "value": 8.0,
            "min": 8.0,
            "max": 8.0,
            "count": 2
        },
        "TetrisAgent.curriculum.board-height.sum": {
            "value": 48.0,
            "min": 40.0,
            "max": 48.0,
            "count": 2
        },
        "TetrisAgent.curriculum.hole-penalty-weight.mean": {
            "value": 0.019999999552965164,
            "min": 0.019999999552965164,
            "max": 0.019999999552965164,
            "count": 2
        },
        "TetrisAgent.curriculum.hole-penalty-weight.sum": {
            "value": 0.11999999731779099,
            "min": 0.09999999776482582,
            "max": 0.11999999731779099,
            "count": 2
        },
        "TetrisAgent.curriculum.tetromino-types.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 2
        },
        "TetrisAgent.curriculum.tetromino-types.sum": {
            "value": 6.0,
            "min": 5.0,
            "max": 6.0,
            "count": 2
        },
        "TetrisAgent.curriculum.board-preset.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 2
        },
        "TetrisAgent.curriculum.board-preset.sum": {
            "value": 6.0,
            "min": 5.0,
            "max": 6.0,
            "count": 2
        },
        "TetrisAgent.metrics.current-holes.mean": {
            "value": 12.833333333333334,
            "min": 11.4,
            "max": 12.833333333333334,
            "count": 2
        },
        "TetrisAgent.metrics.current-holes.sum": {
            "value": 77.0,
            "min": 57.0,
            "max": 77.0,
            "count": 2
        },
        "TetrisAgent.metrics.stack-height.mean": {
            "value": 7.333333333333333,
            "min": 6.6,
            "max": 7.333333333333333,
            "count": 2
        },
        "TetrisAgent.metrics.stack-height.sum": {
            "value": 44.0,
            "min": 33.0,
            "max": 44.0,
            "count": 2
        },
        "TetrisAgent.metrics.surface-roughness.mean": {
            "value": 0.14166666691501936,
            "min": 0.08000000193715096,
            "max": 0.14166666691501936,
            "count": 2
        },
        "TetrisAgent.metrics.surface-roughness.sum": {
            "value": 0.8500000014901161,
            "min": 0.4000000096857548,
            "max": 0.8500000014901161,
            "count": 2
        },
        "TetrisAgent.column-usage.column-0.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-0.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-1.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 1.4,
            "count": 2
        },
        "TetrisAgent.column-usage.column-1.sum": {
            "value": 3.0,
            "min": 3.0,
            "max": 7.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-2.mean": {
            "value": 0.6666666666666666,
            "min": 0.6666666666666666,
            "max": 1.2,
            "count": 2
        },
        "TetrisAgent.column-usage.column-2.sum": {
            "value": 4.0,
            "min": 4.0,
            "max": 6.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-3.mean": {
            "value": 1.5,
            "min": 1.0,
            "max": 1.5,
            "count": 2
        },
        "TetrisAgent.column-usage.column-3.sum": {
            "value": 9.0,
            "min": 5.0,
            "max": 9.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-4.mean": {
            "value": 0.3333333333333333,
            "min": 0.3333333333333333,
            "max": 0.6,
            "count": 2
        },
        "TetrisAgent.column-usage.column-4.sum": {
            "value": 2.0,
            "min": 2.0,
            "max": 3.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-5.mean": {
            "value": 0.16666666666666666,
            "min": 0.16666666666666666,
            "max": 0.6,
            "count": 2
        },
        "TetrisAgent.column-usage.column-5.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 3.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-6.mean": {
            "value": 0.5,
            "min": 0.5,
            "max": 0.8,
            "count": 2
        },
        "TetrisAgent.column-usage.column-6.sum": {
            "value": 3.0,
            "min": 3.0,
            "max": 4.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-7.mean": {
            "value": 1.5,
            "min": 1.0,
            "max": 1.5,
            "count": 2
        },
        "TetrisAgent.column-usage.column-7.sum": {
            "value": 9.0,
            "min": 5.0,
            "max": 9.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-8.mean": {
            "value": 1.3333333333333333,
            "min": 0.8,
            "max": 1.3333333333333333,
            "count": 2
        },
        "TetrisAgent.column-usage.column-8.sum": {
            "value": 8.0,
            "min": 4.0,
            "max": 8.0,
            "count": 2
        },
        "TetrisAgent.column-usage.column-9.mean": {
            "value": 0.3333333333333333,
            "min": 0.0,
            "max": 0.3333333333333333,
            "count": 2
        },
        "TetrisAgent.column-usage.column-9.sum": {
            "value": 2.0,
            "min": 0.0,
            "max": 2.0,
            "count": 2
        },
        "TetrisAgent.action-rewarded.partial-row-fill.mean": {
            "value": 0.0074553476845524505,
            "min": 0.007057344641182382,
            "max": 0.0074553476845524505,
            "count": 2
        },
        "TetrisAgent.action-rewarded.partial-row-fill.sum": {
            "value": 28.717999280896038,
            "min": 17.967999456450343,
            "max": 28.717999280896038,
            "count": 2
        },
        "TetrisAgent.action-rewarded.horizontal-stack.mean": {
            "value": 0.5669236159775753,
            "min": 0.5,
            "max": 0.5669236159775753,
            "count": 2
        },
        "TetrisAgent.action-rewarded.horizontal-stack.sum": {
            "value": 1618.0,
            "min": 1081.0,
            "max": 1618.0,
            "count": 2
        },
        "TetrisAgent.action-rewarded.clear-reward.mean": {
            "value": 5.0,
            "min": 5.0,
            "max": 5.0,
            "count": 2
        },
        "TetrisAgent.action-rewarded.clear-reward.sum": {
            "value": 10.0,
            "min": 10.0,
            "max": 20.0,
            "count": 2
        },
        "TetrisAgent.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 2
        },
        "TetrisAgent.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 2
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1748779883",
        "python_version": "3.9.22 | packaged by conda-forge | (main, Apr 14 2025, 23:36:04) \n[Clang 18.1.8 ]",
        "command_line_arguments": "/opt/anaconda3/envs/ml-unity/bin/mlagents-learn ./Assets/ML-Agents/Config/tetris_ml_config.yaml --run-id=tetris_curriculum_v1 --force",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.11.0",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1748780039"
    },
    "total": 155.916570334,
    "count": 1,
    "self": 0.026695710000012696,
    "children": {
        "run_training.setup": {
            "total": 0.02031704099999998,
            "count": 1,
            "self": 0.02031704099999998
        },
        "TrainerController.start_learning": {
            "total": 155.869557583,
            "count": 1,
            "self": 1.662672826999426,
            "children": {
                "TrainerController._reset_env": {
                    "total": 47.704162125,
                    "count": 1,
                    "self": 47.704162125
                },
                "TrainerController.advance": {
                    "total": 106.28255500500059,
                    "count": 28485,
                    "self": 1.4196518159998703,
                    "children": {
                        "env_step": {
                            "total": 104.86290318900072,
                            "count": 28485,
                            "self": 90.55335513000391,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 14.167338557997184,
                                    "count": 28486,
                                    "self": 0.38265938199673144,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 13.784679176000452,
                                            "count": 28486,
                                            "self": 13.784679176000452
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.14220950099962693,
                                    "count": 28484,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 102.18797507299911,
                                            "count": 28484,
                                            "is_parallel": true,
                                            "self": 25.02842951799815,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.012008999999997272,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0038287089999897717,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0081802910000075,
                                                            "count": 4,
                                                            "is_parallel": true,
                                                            "self": 0.0081802910000075
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 77.14753655500097,
                                                    "count": 28484,
                                                    "is_parallel": true,
                                                    "self": 1.3465007530015498,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 1.509240732999018,
                                                            "count": 28484,
                                                            "is_parallel": true,
                                                            "self": 1.509240732999018
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 71.3205851490005,
                                                            "count": 28484,
                                                            "is_parallel": true,
                                                            "self": 71.3205851490005
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 2.9712099199999136,
                                                            "count": 28484,
                                                            "is_parallel": true,
                                                            "self": 1.5499301749992505,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 1.4212797450006631,
                                                                    "count": 56968,
                                                                    "is_parallel": true,
                                                                    "self": 1.4212797450006631
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 6.633399999600442e-05,
                    "count": 1,
                    "self": 6.633399999600442e-05,
                    "children": {
                        "thread_root": {
                            "total": 0.0,
                            "count": 0,
                            "is_parallel": true,
                            "self": 0.0,
                            "children": {
                                "trainer_advance": {
                                    "total": 101.1967709850071,
                                    "count": 705839,
                                    "is_parallel": true,
                                    "self": 4.478746945008282,
                                    "children": {
                                        "process_trajectory": {
                                            "total": 96.71802403999882,
                                            "count": 705840,
                                            "is_parallel": true,
                                            "self": 96.71802403999882
                                        },
                                        "_update_policy": {
                                            "total": 0.0,
                                            "count": 0,
                                            "is_parallel": true,
                                            "self": 0.0,
                                            "children": {
                                                "TorchPPOOptimizer.update": {
                                                    "total": 2.296153414999935,
                                                    "count": 113,
                                                    "is_parallel": true,
                                                    "self": 2.296153414999935
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.22010129199998119,
                    "count": 1,
                    "self": 0.004123166999988825,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.21597812499999236,
                            "count": 1,
                            "self": 0.21597812499999236
                        }
                    }
                }
            }
        }
    }
}